{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from typing import List\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import nltk\n",
    "import gensim\n",
    "import gensim.downloader as gensim_api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Evaluation for ROUGE score and Semantic Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1. Process output of GeneAgent and GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n",
      "56\n"
     ]
    }
   ],
   "source": [
    "reference = []\n",
    "genes = []\n",
    "# data = pd.read_table(\"NeST_table.tsv\", header=0, index_col=0)\n",
    "data = pd.read_csv(\"Datasets/MsigDB/MsigDB.csv\",header=0, index_col=None)\n",
    "for gene, term in zip(data[\"ID\"], data[\"Name\"]):\n",
    "    term = term.replace('/', ' ').replace(\",\",\" \").replace(\"\\\"\",\"\").replace(\"-\", \" \").strip()\n",
    "    genes.append(gene)\n",
    "    reference.append(term)\n",
    "    \n",
    "print(len(genes))\n",
    "print(len(reference))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt agent file: 1\n",
      "gpt file:1\n"
     ]
    }
   ],
   "source": [
    "def process_text(text: str) -> list:\n",
    "    pattern = r'\\([^)]*\\)'\n",
    "    segments = text.split('//')\n",
    "    # Remove numbers and stop tokens ('-', '*')\n",
    "    cleaned_segments = []\n",
    "    for segment in segments:\n",
    "        cleaned_segment = ''.join(char for char in segment)\n",
    "        cleaned_segment = re.sub(pattern, '', cleaned_segment)\n",
    "        cleaned_segment = cleaned_segment.replace('/', ' ').replace(\",\",\" \").replace(\"\\\"\",\"\").replace(\"-\", \" \").strip()\n",
    "        if cleaned_segment:\n",
    "            cleaned_segments.append(cleaned_segment)\n",
    "\n",
    "    return cleaned_segments\n",
    "\n",
    "## read results of GeneAgent\n",
    "agent = \"\"\n",
    "with open (\"Outputs/GeneAgent/Cascade/MsigDB_Final_Response_GeneAgent.txt\", \"r\") as agentfile:\n",
    "    for line in agentfile.readlines():\n",
    "        agent += line\n",
    "agent_text = process_text(agent)\n",
    "agent_term = []\n",
    "for text in agent_text:\n",
    "    seg = text.split(\"\\n\")\n",
    "    if len(seg) > 1:\n",
    "        agent_term.append(seg[0].split(\": \")[1])\n",
    "    else:\n",
    "        agent_term.append(\"None\")\n",
    "print(\"gpt agent file: %d\" %(len(agent_term)))\n",
    "        \n",
    "## read results of GPT4\n",
    "gpt = \"\"\n",
    "with open (\"Outputs/GPT-4/MsigDB_Response_GPT4.txt\", \"r\") as gptfile:\n",
    "    for line in gptfile.readlines():\n",
    "        gpt += line\n",
    "gpt_text = process_text(gpt)\n",
    "gpt_term = []\n",
    "for text in gpt_text:\n",
    "    seg = text.split(\"\\n\")\n",
    "    if len(seg) > 1:\n",
    "        gpt_term.append(seg[0].split(\": \")[1])\n",
    "    else:\n",
    "        gpt_term.append(\"None\")\n",
    "print(\"gpt file:%d\" %(len(gpt_term)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge-score in c:\\users\\h\\geneagent-openai-1.x-1\\genagentenv\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: transformers in c:\\users\\h\\geneagent-openai-1.x-1\\genagentenv\\lib\\site-packages (4.55.4)\n",
      "Requirement already satisfied: hf_xet in c:\\users\\h\\geneagent-openai-1.x-1\\genagentenv\\lib\\site-packages (1.1.9)\n",
      "Requirement already satisfied: absl-py in c:\\users\\h\\geneagent-openai-1.x-1\\genagentenv\\lib\\site-packages (from rouge-score) (2.3.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\h\\geneagent-openai-1.x-1\\genagentenv\\lib\\site-packages (from rouge-score) (3.9.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\h\\geneagent-openai-1.x-1\\genagentenv\\lib\\site-packages (from rouge-score) (1.26.3)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\h\\geneagent-openai-1.x-1\\genagentenv\\lib\\site-packages (from rouge-score) (1.17.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\h\\geneagent-openai-1.x-1\\genagentenv\\lib\\site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\h\\geneagent-openai-1.x-1\\genagentenv\\lib\\site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\h\\geneagent-openai-1.x-1\\genagentenv\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\h\\geneagent-openai-1.x-1\\genagentenv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\h\\geneagent-openai-1.x-1\\genagentenv\\lib\\site-packages (from transformers) (2025.7.34)\n",
      "Requirement already satisfied: requests in c:\\users\\h\\geneagent-openai-1.x-1\\genagentenv\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\h\\geneagent-openai-1.x-1\\genagentenv\\lib\\site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\h\\geneagent-openai-1.x-1\\genagentenv\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\h\\geneagent-openai-1.x-1\\genagentenv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\h\\geneagent-openai-1.x-1\\genagentenv\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\h\\geneagent-openai-1.x-1\\genagentenv\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\h\\geneagent-openai-1.x-1\\genagentenv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: click in c:\\users\\h\\geneagent-openai-1.x-1\\genagentenv\\lib\\site-packages (from nltk->rouge-score) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\h\\geneagent-openai-1.x-1\\genagentenv\\lib\\site-packages (from nltk->rouge-score) (1.5.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\h\\geneagent-openai-1.x-1\\genagentenv\\lib\\site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\h\\geneagent-openai-1.x-1\\genagentenv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\h\\geneagent-openai-1.x-1\\genagentenv\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\h\\geneagent-openai-1.x-1\\genagentenv\\lib\\site-packages (from requests->transformers) (2025.8.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install rouge-score transformers hf_xet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. Calculate ROUGE scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from rouge_score import rouge_scorer\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tmetrics = [\"rouge1\", \"rouge2\", \"rougeL\"]\n",
    "\tmetric2results = {metric: [] for metric in metrics}\n",
    "\tscorer = rouge_scorer.RougeScorer(metrics, use_stemmer=True)\n",
    " \n",
    "\tfor ref, hypagent in zip (reference, agent_term):\n",
    "    # for ref, hypgpt in zip (reference, gpt_term):\n",
    "\t\tscores_agent = scorer.score(ref, hypagent)\n",
    "\t\t# scores_gpt = scorer.score(ref, hypgpt)\n",
    "\n",
    "\t\tfor metric in metrics:\n",
    "\t\t\tmetric2results[metric].append(scores_agent[metric].fmeasure)\n",
    "   \n",
    "\tf = open(\"MsigDB.Rouge.txt\",\"a\")\n",
    "\tf.write(\"\\n====GeneAgent (Cascade)====\\n\")\n",
    "\tfor metric in metrics:\n",
    "\t\tresults = metric2results[metric]\n",
    "\t\tf.write(metric + \":\" + str(sum(results) / len(results)) + \"\\n\")\n",
    "\tf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3. Calculate semantic similarity using MedCPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\h\\GeneAgent-openai-1.x-1\\GenAgentEnv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5798805952072144 0.47307318449020386\n",
      "0.5798805952072144 0.47307318449020386\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "model = AutoModel.from_pretrained(\"ncbi/MedCPT-Query-Encoder\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ncbi/MedCPT-Query-Encoder\")\n",
    "\n",
    "def cos_sim(a: Tensor, b: Tensor):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\n",
    "    :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\n",
    "    \"\"\"\n",
    "    if not isinstance(a, torch.Tensor):\n",
    "        a = torch.tensor(a)\n",
    "\n",
    "    if not isinstance(b, torch.Tensor):\n",
    "        b = torch.tensor(b)\n",
    "\n",
    "    if len(a.shape) == 1:\n",
    "        a = a.unsqueeze(0)\n",
    "\n",
    "    if len(b.shape) == 1:\n",
    "        b = b.unsqueeze(0)\n",
    "\n",
    "    a_norm = torch.nn.functional.normalize(a, p=2, dim=1)\n",
    "    b_norm = torch.nn.functional.normalize(b, p=2, dim=1)\n",
    "    return torch.mm(a_norm, b_norm.transpose(0, 1))\n",
    "\n",
    "agent_scores = []\n",
    "gpt_scores = []\n",
    "summary_scores = []\n",
    "\n",
    "for ref, hypagent, hypgpt in zip(reference, agent_term, gpt_term):\n",
    "    with torch.no_grad():\n",
    "        # tokenize the queries\n",
    "        encoded_agent = tokenizer(\n",
    "            [ref, hypagent], \n",
    "            truncation=True, \n",
    "            padding=True, \n",
    "            return_tensors='pt', \n",
    "            max_length=64,\n",
    "        )\n",
    "        encoded_gpt = tokenizer(\n",
    "            [ref, hypgpt], \n",
    "            truncation=True, \n",
    "            padding=True, \n",
    "            return_tensors='pt', \n",
    "            max_length=64,\n",
    "        )\n",
    "        \n",
    "        # encode the queries (use the [CLS] last hidden states as the representations)\n",
    "        embeds_agent = model(**encoded_agent).last_hidden_state[:, 0, :] \n",
    "        score_agent = cos_sim(embeds_agent[0], embeds_agent[1])\n",
    "        agent_scores.append(score_agent.tolist()[0])\n",
    "        \n",
    "        embeds_gpt = model(**encoded_gpt).last_hidden_state[:, 0, :]\n",
    "        score_gpt = cos_sim(embeds_gpt[0], embeds_gpt[1])\n",
    "        gpt_scores.append(score_gpt.tolist()[0])\n",
    "        \n",
    "        \n",
    "print(np.average(agent_scores),np.average(gpt_scores))\n",
    "print(np.max(agent_scores),np.max(gpt_scores))  \n",
    "      \n",
    "np.savetxt(\"MsigDB.GeneAgent.Cascade.Semantic.csv\", np.asarray(agent_scores), fmt=\"%s\", delimiter=\"\\t\", newline=\"\\n\") \n",
    "np.savetxt(\"MsigDB.GPT4.Semantic.csv\", np.asarray(gpt_scores), fmt=\"%s\", delimiter=\"\\t\", newline=\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Evaluation for background semantic similarity distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1. Collect the background gene sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12214\n",
      "12264\n",
      "12320\n",
      "12320\n"
     ]
    }
   ],
   "source": [
    "GSS = []\n",
    "\n",
    "index = 0\n",
    "bp = pd.read_csv(\"BP_terms_All.csv\", header=0, index_col=0)\n",
    "for ID, Genes, Count, Name in zip(bp[\"GO\"], bp[\"Genes\"], bp[\"Gene_Count\"], bp[\"Truth Label\"]):\n",
    "    GSS.append([index, ID, Genes, Count, Name])\n",
    "    index += 1\n",
    "        \n",
    "print(len(GSS))\n",
    "\n",
    "with open(\"Datasets/NeST/NeST.tsv\", \"r\") as nestfile:\n",
    "    for line in nestfile.readlines()[1:]:\n",
    "        arr = line.split(\"\\t\")\n",
    "        ID = arr[0]        # NEST ID\n",
    "        Name = arr[1]      # name_new  \n",
    "        Genes = arr[2].replace('\"', '').replace(',', ' ').strip()  # Genes\n",
    "        Count = len(Genes.split())\n",
    "        GSS.append([index, ID, Genes, Count, Name])\n",
    "        index += 1\n",
    "        \n",
    "print(len(GSS))\n",
    "\n",
    "\n",
    "zen = pd.read_csv(\"Datasets/MsigDB/MsigDB.csv\", header=0, index_col=None)\n",
    "for ID, Genes, Count, Name in zip(zen[\"ID\"], zen[\"Genes\"], zen[\"Count\"], zen[\"Name\"]):\n",
    "    GSS.append([index, ID, Genes, Count, Name])\n",
    "    index += 1\n",
    "        \n",
    "print(len(GSS))\n",
    "print(index)\n",
    "\n",
    "\n",
    "with open(\"background.csv\", mode='w', newline='\\n', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file, quoting=csv.QUOTE_ALL)  \n",
    "    writer.writerow(['Index', 'ID', 'Genes', 'Count', 'Term'])\n",
    "    for term in GSS:\n",
    "        writer.writerow(term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. Calculate relative similarity using MedCPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12320\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Count_x</th>\n",
       "      <th>Genes_x</th>\n",
       "      <th>Genes_y</th>\n",
       "      <th>Count_y</th>\n",
       "      <th>Term</th>\n",
       "      <th>Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MsigDB:70</td>\n",
       "      <td>Progeria</td>\n",
       "      <td>4</td>\n",
       "      <td>ZMPSTE24 BANF1 WRN LMNA</td>\n",
       "      <td>ZMPSTE24 BANF1 WRN LMNA</td>\n",
       "      <td>4</td>\n",
       "      <td>Progeria</td>\n",
       "      <td>12264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MsigDB:69</td>\n",
       "      <td>Peroxisome</td>\n",
       "      <td>8</td>\n",
       "      <td>PEX1 PEX2 PEX3 PEX4 PEX5 PEX6 PEX7 PEX8</td>\n",
       "      <td>PEX1 PEX2 PEX3 PEX4 PEX5 PEX6 PEX7 PEX8</td>\n",
       "      <td>8</td>\n",
       "      <td>Peroxisome</td>\n",
       "      <td>12265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MsigDB:4</td>\n",
       "      <td>Glycolysis Gocam</td>\n",
       "      <td>10</td>\n",
       "      <td>HK1 GPI PFKM ALDOA TPI1 GAPDH PGK1 PGAM2 ENO3 PKM</td>\n",
       "      <td>HK1 GPI PFKM ALDOA TPI1 GAPDH PGK1 PGAM2 ENO3 PKM</td>\n",
       "      <td>10</td>\n",
       "      <td>Glycolysis Gocam</td>\n",
       "      <td>12266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MsigDB:72</td>\n",
       "      <td>Sensory Ataxia</td>\n",
       "      <td>15</td>\n",
       "      <td>EGR2 NAGLU GPI DNAJC3 SH3TC2 TWNK PIEZO2 FLVCR...</td>\n",
       "      <td>EGR2 NAGLU GPI DNAJC3 SH3TC2 TWNK PIEZO2 FLVCR...</td>\n",
       "      <td>15</td>\n",
       "      <td>Sensory Ataxia</td>\n",
       "      <td>12267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MsigDB:6</td>\n",
       "      <td>Endocytosis</td>\n",
       "      <td>16</td>\n",
       "      <td>AHSG CARMIL1 DNM2 DOCK2 EHD4 MAPKAPK2 MAPKAPK3...</td>\n",
       "      <td>AHSG CARMIL1 DNM2 DOCK2 EHD4 MAPKAPK2 MAPKAPK3...</td>\n",
       "      <td>16</td>\n",
       "      <td>Endocytosis</td>\n",
       "      <td>12268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID              Name  Count_x  \\\n",
       "0  MsigDB:70          Progeria        4   \n",
       "1  MsigDB:69        Peroxisome        8   \n",
       "2   MsigDB:4  Glycolysis Gocam       10   \n",
       "3  MsigDB:72    Sensory Ataxia       15   \n",
       "4   MsigDB:6       Endocytosis       16   \n",
       "\n",
       "                                             Genes_x  \\\n",
       "0                            ZMPSTE24 BANF1 WRN LMNA   \n",
       "1            PEX1 PEX2 PEX3 PEX4 PEX5 PEX6 PEX7 PEX8   \n",
       "2  HK1 GPI PFKM ALDOA TPI1 GAPDH PGK1 PGAM2 ENO3 PKM   \n",
       "3  EGR2 NAGLU GPI DNAJC3 SH3TC2 TWNK PIEZO2 FLVCR...   \n",
       "4  AHSG CARMIL1 DNM2 DOCK2 EHD4 MAPKAPK2 MAPKAPK3...   \n",
       "\n",
       "                                             Genes_y  Count_y  \\\n",
       "0                            ZMPSTE24 BANF1 WRN LMNA        4   \n",
       "1            PEX1 PEX2 PEX3 PEX4 PEX5 PEX6 PEX7 PEX8        8   \n",
       "2  HK1 GPI PFKM ALDOA TPI1 GAPDH PGK1 PGAM2 ENO3 PKM       10   \n",
       "3  EGR2 NAGLU GPI DNAJC3 SH3TC2 TWNK PIEZO2 FLVCR...       15   \n",
       "4  AHSG CARMIL1 DNM2 DOCK2 EHD4 MAPKAPK2 MAPKAPK3...       16   \n",
       "\n",
       "               Term  Index  \n",
       "0          Progeria  12264  \n",
       "1        Peroxisome  12265  \n",
       "2  Glycolysis Gocam  12266  \n",
       "3    Sensory Ataxia  12267  \n",
       "4       Endocytosis  12268  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "back = pd.read_csv(\"background.csv\", header=0, index_col=0)\n",
    "back[\"Index\"] = back.index\n",
    "\n",
    "all_Ref = []\n",
    "for term in back[\"Term\"]:\n",
    "    all_Ref.append(term)\n",
    "print(len(all_Ref))\n",
    "\n",
    "data = pd.read_csv(\"Datasets/MsigDB/MsigDB.csv\", header=0, index_col=None)\n",
    "AllDATA = pd.merge(data, back, on='ID', how='inner')\n",
    "AllDATA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MedCPT model...\n",
      "Using device: cuda:0\n",
      "Processing embeddings with CUDA optimization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch 770/770: 100%|██████████| 770/770 [04:06<00:00,  3.12batch/s]\n",
      "Processing batch 1/1: 100%|██████████| 1/1 [00:00<00:00,  3.43batch/s]\n",
      "Processing batch 1/1: 100%|██████████| 1/1 [00:00<00:00,  3.63batch/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# CUDA 设备配置\n",
    "print(\"Loading MedCPT model...\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.set_device(0)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 加载模型并移到 GPU\n",
    "model = AutoModel.from_pretrained(\"ncbi/MedCPT-Query-Encoder\")\n",
    "model = model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ncbi/MedCPT-Query-Encoder\")\n",
    "\n",
    "def get_medcpt_embeddings_batch(queries, batch_size=32):\n",
    "    \"\"\"\n",
    "    批处理版本，支持 CUDA 并包含内存管理和进度条\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Create progress bar\n",
    "        total_batches = (len(queries) - 1) // batch_size + 1\n",
    "        pbar = tqdm(range(0, len(queries), batch_size), \n",
    "                   desc=\"Processing batches\", \n",
    "                   total=total_batches,\n",
    "                   unit=\"batch\")\n",
    "        \n",
    "        for i in pbar:\n",
    "            batch_queries = queries[i:i+batch_size]\n",
    "            batch_num = i // batch_size + 1\n",
    "            pbar.set_description(f\"Processing batch {batch_num}/{total_batches}\")\n",
    "            \n",
    "            encoded = tokenizer(\n",
    "                batch_queries, \n",
    "                truncation=True, \n",
    "                padding=True, \n",
    "                return_tensors='pt', \n",
    "                max_length=64,\n",
    "            )\n",
    "            \n",
    "            # 将输入张量移到与模型相同的设备\n",
    "            encoded = {k: v.to(device) for k, v in encoded.items()}\n",
    "            \n",
    "            embeds = model(**encoded).last_hidden_state[:, 0, :]\n",
    "            all_embeddings.append(embeds.cpu())  # 移回 CPU 节省显存\n",
    "            \n",
    "            # 清理 GPU 缓存\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            # 强制垃圾回收\n",
    "            gc.collect()\n",
    "    \n",
    "    # 连接所有嵌入\n",
    "    final_embeddings = torch.cat(all_embeddings, dim=0)\n",
    "    return final_embeddings, final_embeddings.size()\n",
    "\n",
    "# 使用批处理版本处理嵌入\n",
    "print(\"Processing embeddings with CUDA optimization...\")\n",
    "ref_embeds, ref_embeds_size = get_medcpt_embeddings_batch(all_Ref, batch_size=16)\n",
    "agent_embeds, agent_embeds_size = get_medcpt_embeddings_batch(agent_term, batch_size=32)\n",
    "gpt_embeds, gpt_embeds_size = get_medcpt_embeddings_batch(gpt_term, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 12320)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/56 [00:00<00:00, 123.22it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m row = \u001b[32m0\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ind \u001b[38;5;129;01min\u001b[39;00m tqdm(AllDATA[\u001b[33m\"\u001b[39m\u001b[33mIndex\u001b[39m\u001b[33m\"\u001b[39m]):\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     root = \u001b[43mscores\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m]\u001b[49m[ind]\n\u001b[32m     19\u001b[39m     ith = \u001b[32m1\u001b[39m\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(scores.shape[\u001b[32m1\u001b[39m]):\n",
      "\u001b[31mIndexError\u001b[39m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "cos = torch.nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "\n",
    "relative = []\n",
    "for gpt_tensor in tqdm(gpt_embeds):\n",
    "    temp = []\n",
    "    for ref_tensor in ref_embeds:\n",
    "        score = cos(gpt_tensor, ref_tensor)\n",
    "        temp.append(score.tolist())\n",
    "\n",
    "    relative.append(temp)\n",
    "\n",
    "scores = np.asarray(relative)\n",
    "print (scores.shape)\n",
    "        \n",
    "rank = []\n",
    "row = 0\n",
    "for ind in tqdm(AllDATA[\"Index\"]):\n",
    "    root = scores[row][ind]\n",
    "    ith = 1\n",
    "    for j in range(scores.shape[1]):\n",
    "        if scores[row][j] > root:\n",
    "            ith += 1 \n",
    "    rank.append(ith)\n",
    "    row += 1\n",
    "    \n",
    "np.savetxt(\"MsigDB.Relative.Rank.GPT.Background.txt\", np.asarray(rank), fmt=\"%s\", newline=\"\\n\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluation for multiple enrichment terms test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1. Process output of GPT-4 in summarizing multiple enrichment terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Outputs/EnrichedTermTest/gpt.geneagent.msigdb.summary.result.verification.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m text = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mOutputs/EnrichedTermTest/gpt.geneagent.msigdb.summary.result.verification.txt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m summary:\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m summary.readlines():\n\u001b[32m      4\u001b[39m         text += line\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\h\\GeneAgent-openai-1.x-1\\GenAgentEnv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'Outputs/EnrichedTermTest/gpt.geneagent.msigdb.summary.result.verification.txt'"
     ]
    }
   ],
   "source": [
    "text = \"\"\n",
    "with open(\"Outputs/EnrichedTermTest/gpt.geneagent.msigdb.summary.result.verification.txt\") as summary:\n",
    "    for line in summary.readlines():\n",
    "        text += line\n",
    "segments = text.split('//')\n",
    "print(len(segments))\n",
    "enrich_terms = []\n",
    "for segment in segments:\n",
    "    cleaned_segment = ''.join(char for char in segment)\n",
    "    enrich = cleaned_segment.split(\"\\n\\n\")[-2].replace(\".\", \"\").replace(\"\\n\",\"\")\n",
    "    enrich_terms.append(enrich.split(\"Enriched Terms: \")[1].split(\"; \"))\n",
    "    \n",
    "print(len(enrich_terms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2. Exact match with all significant enrichment terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"GSEATerms/MsigDB.EnrichTerms.Allsignificant.json\",\"r\") as file:\n",
    "    enrich = json.load(file)\n",
    "    \n",
    "name2id = {}\n",
    "for names in enrich:\n",
    "    for name in names:\n",
    "        if name[\"name\"].lower() in name2id.keys():\n",
    "            name2id[name[\"name\"].lower()].append(name[\"native\"])\n",
    "        else:\n",
    "            name2id[name[\"name\"].lower()] = [name[\"native\"]]\n",
    "\n",
    "results = []\n",
    "for terms in enrich_terms:\n",
    "    matched = {} \n",
    "    for term in terms:\n",
    "        if term.lower() in name2id.keys():\n",
    "            matched[term] = list(set(name2id[term.lower()]))\n",
    "        else:\n",
    "            matched[term] = \"None\"\n",
    "\n",
    "    results.append(matched)\n",
    "        \n",
    "with open(\"Term2Enrich_Exact.Verification.Allsignificant.json\", \"w\") as file:\n",
    "    json.dump(results, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Term2Enrich_Exact.Verification.Allsignificant.json\",\"r\") as enrichfile:\n",
    "    data = json.load(enrichfile)\n",
    "print(len(data))\n",
    "\n",
    "total, success, fail = 0, 0, 0\n",
    "for terms in data:\n",
    "    for key in terms.keys():\n",
    "        total += 1\n",
    "        if terms[key] != \"None\":\n",
    "            success += 1\n",
    "        else:\n",
    "            fail += 1\n",
    "\n",
    "print(f\"the total number of summarized terms: {total}\")\n",
    "print(f\"the successful number of summarized terms: {success}\")\n",
    "print(f\"the failed number of summarized terms: {fail}\")\n",
    "print(f\"the match rate of summarized terms: {float(success/total)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-3. Exact match with top-k (k=1,3,5) significant enrichment terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exact MATCH with ENRICHED TERMS in TOP 1\n",
    "with open(\"GSEATerms/MsigDB.EnrichTerms.top1.json\",\"r\") as file:\n",
    "    enrich = json.load(file)\n",
    "\n",
    "for ind in range(0, len(enrich_terms)):\n",
    "    for pos in range(0, len(enrich_terms[ind])):\n",
    "        enrich_terms[ind][pos] = enrich_terms[ind][pos].lower().replace(\"-\",\" \")\n",
    "\n",
    "pairs = []\n",
    "for terms, data in zip(enrich_terms, enrich):\n",
    "    temp = {}\n",
    "    if data[\"name\"].lower().replace(\"-\",\" \") in terms:\n",
    "        temp[data[\"name\"]] = data[\"native\"]\n",
    "    else:\n",
    "        temp[data[\"name\"]] = \"None\"\n",
    "        \n",
    "    pairs.append(temp)\n",
    "    \n",
    "print(len(pairs))\n",
    "with open(\"Term2Enrich_Exact.Verification.top1.json\", \"w\") as file:\n",
    "    json.dump(pairs, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exact MATCH with ENRICHED TERMS in TOP 3 and TOP 5\n",
    "with open(\"GSEATerms/MsigDB.EnrichTerms.top5.json\",\"r\") as file:\n",
    "    enrich = json.load(file)\n",
    "    \n",
    "name2id = {}\n",
    "for names in enrich:\n",
    "    for name in names:\n",
    "        if name[\"name\"].lower() in name2id.keys():\n",
    "            name2id[name[\"name\"].lower()].append(name[\"native\"])\n",
    "        else:\n",
    "            name2id[name[\"name\"].lower()] = [name[\"native\"]]\n",
    "    \n",
    "results = []\n",
    "for terms in enrich_terms:\n",
    "    matched = {} \n",
    "    for term in terms:\n",
    "        if term.lower() in name2id.keys():\n",
    "            matched[term] = list(set(name2id[term.lower()]))\n",
    "        else:\n",
    "            matched[term] = \"None\"\n",
    "\n",
    "    results.append(matched)\n",
    "        \n",
    "with open(\"Term2Enrich_Exact.Verification.Top5.json\", \"w\") as file:\n",
    "    json.dump(results, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Term2Enrich_Exact.Verification.Top5.json\",\"r\") as gofile:\n",
    "    data = json.load(gofile)\n",
    "print(len(data))\n",
    "\n",
    "total, success, fail = 0, 0, 0\n",
    "for terms in data:\n",
    "    total += 1\n",
    "    for key in terms.keys():\n",
    "        if terms[key] != \"None\":\n",
    "            success += 1\n",
    "            break\n",
    "        else:\n",
    "            # fail += 1\n",
    "            continue\n",
    "\n",
    "print(f\"the total number of summarized terms: {total}\")\n",
    "print(f\"the successful number of summarized terms: {success}\")\n",
    "print(f\"the failed number of summarized terms: {total - success}\")\n",
    "print(f\"the match rate of summarized terms: {float(success/total)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Other BERT-based model for the evaluation of semantic similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SENTENCE BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\h\\GeneAgent-openai-1.x-1\\GenAgentEnv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\h\\GeneAgent-openai-1.x-1\\GenAgentEnv\\Lib\\site-packages\\jieba\\_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "c:\\Users\\h\\GeneAgent-openai-1.x-1\\GenAgentEnv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\h\\.cache\\huggingface\\hub\\models--shibing624--text2vec-base-chinese. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'reference' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m scores = []\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# for ref, hyp in zip(reference, hypothesis_term):\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ref, hyp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[43mreference\u001b[49m, agent_term):\n\u001b[32m     12\u001b[39m   score = sim_model.get_score(ref, hyp)\n\u001b[32m     13\u001b[39m   scores.append(score)\n",
      "\u001b[31mNameError\u001b[39m: name 'reference' is not defined"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "  dev = \"cuda:0\"\n",
    "else:\n",
    "  dev = \"cpu\"\n",
    "torch.cuda.set_device(dev)\n",
    "\n",
    "from text2vec import Similarity\n",
    "sim_model = Similarity()\n",
    "scores = []\n",
    "# for ref, hyp in zip(reference, hypothesis_term):\n",
    "for ref, hyp in zip(reference, agent_term):\n",
    "  score = sim_model.get_score(ref, hyp)\n",
    "  scores.append(score)\n",
    "\n",
    "print(np.average(scores),np.max(scores))\n",
    "# np.savetxt(\"semantic_similarity_nest_enrichment.txt\", np.asarray(scores), fmt=\"%s\", delimiter=\"\\t\\t\", newline=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SapBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel  \n",
    "\n",
    "def cos_sim(a: Tensor, b: Tensor):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\n",
    "    :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\n",
    "    \"\"\"\n",
    "    if not isinstance(a, torch.Tensor):\n",
    "        a = torch.tensor(a)\n",
    "\n",
    "    if not isinstance(b, torch.Tensor):\n",
    "        b = torch.tensor(b)\n",
    "\n",
    "    if len(a.shape) == 1:\n",
    "        a = a.unsqueeze(0)\n",
    "\n",
    "    if len(b.shape) == 1:\n",
    "        b = b.unsqueeze(0)\n",
    "\n",
    "    a_norm = torch.nn.functional.normalize(a, p=2, dim=1)\n",
    "    b_norm = torch.nn.functional.normalize(b, p=2, dim=1)\n",
    "    return torch.mm(a_norm, b_norm.transpose(0, 1))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cambridgeltl/SapBERT-from-PubMedBERT-fulltext\")  \n",
    "model = AutoModel.from_pretrained(\"cambridgeltl/SapBERT-from-PubMedBERT-fulltext\").cuda()\n",
    "\n",
    "# replace with your own list of entity names\n",
    "# all_names = [\"covid-19\", \"Coronavirus infection\", \"high fever\", \"Tumor of posterior wall of oropharynx\"] \n",
    "\n",
    "# bs = 128 # batch size during inference\n",
    "# all_embs = []\n",
    "# for i in tqdm(np.arange(0, len(all_names), bs)):\n",
    "scores = []\n",
    "for ref, hyp in zip(reference, gpt_term):\n",
    "    toks = tokenizer.batch_encode_plus([ref, hyp], \n",
    "                                       padding=\"max_length\", \n",
    "                                       max_length=30, \n",
    "                                       truncation=True,\n",
    "                                       return_tensors=\"pt\")\n",
    "    toks_cuda = {}\n",
    "    for k,v in toks.items():\n",
    "        toks_cuda[k] = v.cuda()\n",
    "    cls_rep = model(**toks_cuda)[0][:,0,:] # use CLS representation as the embedding\n",
    "    score = cos_sim(cls_rep[0], cls_rep[1])\n",
    "    scores.append(score.tolist()[0])\n",
    "    \n",
    "print(np.average(scores),np.max(scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenAgentEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
